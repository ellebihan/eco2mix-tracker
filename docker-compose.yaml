services:
  db:
    image: postgres:17.3
    container_name: dbelsa
    environment:
      POSTGRES_USER: ${PG_DB_USER}
      POSTGRES_PASSWORD: ${PG_DB_PWD}
      POSTGRES_DB: ${PG_DB_NAME}
    volumes:
      - db_main_data:/var/lib/postgresql/data
    ports:
      - "5433:5432"
    networks:
      - app-network   # ← ajoute ceci

  # 🔶 Base dédiée à Airflow
  db-airflow:
    image: postgres:17.3
    container_name: db_airflow
    environment:
      POSTGRES_USER: ${PG_DB_AIRFLOW_USER}
      POSTGRES_PASSWORD: ${PG_DB_AIRFLOW_PWD}
      POSTGRES_DB: ${PG_DB_AIRFLOW}
    ports:
      - "5434:5432"
    volumes:
      - db_airflow_data:/var/lib/postgresql/data
    networks:
      - app-network

  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile
    image: ${AIRFLOW_IMAGE}
    depends_on:
      - db-airflow
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_EXECUTOR}
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@db-airflow/airflow
    volumes:
      - .:/opt/airflow

    entrypoint: ["airflow", "db", "init"]
    networks:
      - app-network

  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile
    image: ${AIRFLOW_IMAGE}
    depends_on:
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_EXECUTOR}
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@db-airflow/airflow
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY}
      PYTHONPATH: ${PYTHON_PATH}
    volumes:
      - ./dags:/opt/airflow/dags                  # tes DAGs Airflow
      - ./dbt_elsa:/opt/airflow/dbt_elsa          # ton projet dbt (avec dbt_project.yml)
      - ./dbt_elsa/profiles.yml:/home/airflow/.dbt/profiles.yml  # fichier profiles.yml attendu par dbt
      - ./scripts:/opt/airflow/scripts            # si tu as des scripts Python à exécuter
      - ./common:/opt/airflow/common            # d'autres scripts Python à exécuter
      - ./datasources.yaml:/opt/airflow/datasources.yaml  # les datasources
    ports:
      - "8080:8080"
    command: webserver
    networks:
      - app-network

  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile
    image: ${AIRFLOW_IMAGE}
    depends_on:
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_EXECUTOR}
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@db-airflow/airflow
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY}
      PYTHONPATH: ${PYTHON_PATH}
    volumes:
      - ./dags:/opt/airflow/dags  # tes DAGs Airflow
      - ./dbt_elsa:/opt/airflow/dbt_elsa # ton projet dbt (avec dbt_project.yml)
      - ./dbt_elsa/profiles.yml:/home/airflow/.dbt/profiles.yml # fichier profiles.yml attendu par dbt
      - ./scripts:/opt/airflow/scripts  # si tu as des scripts Python à exécuter
      - ./common:/opt/airflow/common  # d'autres scripts Python à exécuter
      - ./datasources.yaml:/opt/airflow/datasources.yaml  # les datasources
    command: scheduler
    networks:
      - app-network

  superset:
    image: apache/superset
    container_name: superset
    environment:
      - SUPERSET_SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY}
    ports:
      - "8088:8088"
    volumes:
      - superset_home:/app/superset_home
    command: >
      /bin/bash -c "
      superset db upgrade &&
      superset fab create-admin --username admin --firstname Superset --lastname Admin --email admin@superset.com --password admin &&
      superset init &&
      superset run -h 0.0.0.0 -p 8088
      "

volumes:
  db_main_data:
  db_airflow_data:
  superset_home:

networks:
  app-network: