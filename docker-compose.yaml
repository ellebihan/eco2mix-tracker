services:
  db:
    image: postgres:17.3
    container_name: dbelsa
    environment:
      POSTGRES_USER: ${PG_DB_USER}
      POSTGRES_PASSWORD: ${PG_DB_PWD}
      POSTGRES_DB: ${PG_DB_NAME}
    volumes:
      - db_main_data:/var/lib/postgresql/data
    ports:
      - "5433:5432"
    networks:
      - app-network   # ‚Üê ajoute ceci

  # üî∂ Base d√©di√©e √† Airflow
  db-airflow:
    image: postgres:17.3
    container_name: db_airflow
    environment:
      POSTGRES_USER: ${PG_DB_AIRFLOW_USER}
      POSTGRES_PASSWORD: ${PG_DB_AIRFLOW_PWD}
      POSTGRES_DB: ${PG_DB_AIRFLOW}
    ports:
      - "5434:5432"
    volumes:
      - db_airflow_data:/var/lib/postgresql/data
    networks:
      - app-network

  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile
    image: ${AIRFLOW_IMAGE}
    depends_on:
      - db-airflow
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_EXECUTOR}
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@db-airflow/airflow
    volumes:
      - .:/opt/airflow

    entrypoint: ["airflow", "db", "init"]
    networks:
      - app-network

  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile
    image: ${AIRFLOW_IMAGE}
    depends_on:
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_EXECUTOR}
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@db-airflow/airflow
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"
      PYTHONPATH: ${PYTHON_PATH}
    volumes:
      - ./dags:/opt/airflow/dags                  # tes DAGs Airflow
      - ./dbt_elsa:/opt/airflow/dbt_elsa          # ton projet dbt (avec dbt_project.yml)
      - ./dbt_elsa/profiles.yml:/home/airflow/.dbt/profiles.yml  # fichier profiles.yml attendu par dbt
      - ./scripts:/opt/airflow/scripts            # si tu as des scripts Python √† ex√©cuter
    ports:
      - "8080:8080"
    command: webserver
    networks:
      - app-network

  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile
    image: ${AIRFLOW_IMAGE}
    depends_on:
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW_EXECUTOR}
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@db-airflow/airflow
      PYTHONPATH: ${PYTHON_PATH}
    volumes:
      - ./dags:/opt/airflow/dags                  # tes DAGs Airflow
      - ./dbt_elsa:/opt/airflow/dbt_elsa          # ton projet dbt (avec dbt_project.yml)
      - ./dbt_elsa/profiles.yml:/home/airflow/.dbt/profiles.yml  # fichier profiles.yml attendu par dbt
      - ./scripts:/opt/airflow/scripts            # si tu as des scripts Python √† ex√©cuter
    command: scheduler
    networks:
      - app-network

volumes:
  db_main_data:
  db_airflow_data:

networks:
  app-network: